Exercise 9.2

a. We regularize with respect to sparsity because more sparsity means a smaller model. This means it makes some values
useless while making others very useful. The benefits of this are that the model is more efficient because there are
less weights to deal with, and it is better at avoiding overfitting because it is again tuning less weights.
b. L1 regularization increases sparsity because it makes some features 0 while selecting to keep the useful features.
It does this based on the L1 formula, where the absolute value involved causes some constant value to be subtracted
from the weight every time. This also means L1 has discontinuity at 0, so any subtraction results that cross 0 are
zeroed out.
c. We had to make the regularization strength 0.8 before the model satisfied the restraints of model size < 600 and
LogLoss < 0.35. The final results were model size = 583 and LogLoss = 0.25.