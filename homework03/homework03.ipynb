{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1\n",
    "\n",
    "The calculations below show the information gain of asking the \"Price\" question using the restaurant data from AIMA\n",
    "Figure 18.3.\n",
    "\n",
    "Entropy(Restaurant) = -[(6/12)lg(6/12) + (6/12)lg(6/12)]\n",
    " \n",
    "Entropy(Restaurant) = 1.0\n",
    "\n",
    "Remainder(Price?) = (7/12)Entropy($) + (2/12)Entropy($$) + (3/12)Entropy($$$)\n",
    "\n",
    "Remainder(Price?) = -(7/12)[(4/7)lg(4/7) + (3/7)lg(3/7)] + -(2/12)[(0/2)lg(0/2) + (2/2)lg(2/2)] + -(3/12)[(2/3)lg(2/3) + (1/3)lg(1/3)]\n",
    "\n",
    "Remainder(Price?) = 0.804\n",
    "\n",
    "Gain(Price?) = Entropy(Restaurant) - Remainder(Price?)\n",
    "\n",
    "Gain(Price?) = 1.0 - 0.804\n",
    "\n",
    "Gain(Price?) = 0.196\n",
    "\n",
    "Asking the \"Price\" question is better than asking the \"Type\" question but worse than asking the \"Patron\" question because\n",
    "of the information gain that each of them produce. The \"Price\" question yields a gain of 0.196, while the \"Type\" question\n",
    "yields a gain of 0 and the \"Patron\" question yields a gain of 0.54."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2\n",
    "\n",
    "In class a neural network that computes the XOR function was constructed. This network took 2 inputs and deeply\n",
    "connected them to a hidden layer of 2 nodes, 1 being an AND gate and the other being an OR gate. A bias was also deeply\n",
    "connected to these nodes. The outputs of these 2 nodes and another bias all combined to generate an output that\n",
    "functioned like an XOR. Upon further analysis, this network was simplified while still resulting in a functioning XOR \n",
    "gate. This model takes 2 inputs, with both inputs going to a hidden node as well as too the output node. The weights\n",
    "as these inputs go to the hidden node are both 0.75, and their weights as they go directly to the output node are both\n",
    "1.5. The hidden node has a threshold of 1, and the weight of its output that inputs to the output node is -4. The output\n",
    "node gets these 3 inputs, and with a threshold of 1 it successfully acts as a XOR gate without the biases and extra \n",
    "hidden node originally used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3\n",
    "\n",
    "Python, NumPy, Pandas, and Keras were all used to manipulate the Boston Housing dataset. The dimensions of the training\n",
    "set was 404x13 while the testing set was 102x13. This was found with the code shown below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "training data         \n",
      "\tcount: 404         \n",
      "\tshape: (404, 13)         \n",
      "\ttype: float64\n",
      " testing data         \n",
      "\tcount: 102         \n",
      "\tshape: (102, 13)        \n",
      "\ttype: float64\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\n",
    "\n",
    "print(\n",
    "    f'training data \\\n",
    "        \\n\\tcount: {len(train_targets)} \\\n",
    "        \\n\\tshape: {train_data.shape} \\\n",
    "        \\n\\ttype: {train_targets.dtype}\\n',\n",
    "    f'testing data \\\n",
    "        \\n\\tcount: {len(test_targets)} \\\n",
    "        \\n\\tshape: {test_data.shape}\\\n",
    "        \\n\\ttype: {test_targets.dtype}\\n',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This data could be converted into testing, training, and validation sets using Pandas. Since the testing data was\n",
    "already separated, it could simply be turned into a testing dataframe. In order to separate the larger training set into\n",
    "training and validations sets, the training set was turned into an initial dataframe. From this dataframe, head and \n",
    "tail functions were used to split the data into a validation set and smaller training set. This code is shown below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__\n",
    "\n",
    "testing = pd.DataFrame(test_data).copy()\n",
    "training1 = pd.DataFrame(train_data).copy()\n",
    "validation = training1.tail(102).copy()\n",
    "training2 = training1.head(302).copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A useful synthetic feature that could be created is dividing the weighted distances to five Boston employment centres\n",
    "[7] by the per capita crime rate [0]. This would be useful in showing how the crime rate is affected by how far an \n",
    "area is from the populated employment centres. Therefore, the combination of these features would allow the model to  \n",
    "more accurately predict the housing price because it would have a better understanding of the crime trends in areas \n",
    "relative to where people are working (which affects the price). The creation of this feature is shown below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "testing[\"area of crime\"] = testing[7] / testing[0]\n",
    "training2[\"area of crime\"] = training2[7] / training2[0]\n",
    "validation[\"area of crime\"] = validation[7] / validation[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}