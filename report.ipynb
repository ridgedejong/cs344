{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHB8jTF/60kYykFhwgAhln"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKD_quYvcPbP",
        "colab_type": "text"
      },
      "source": [
        "## Final Report: Deep Dream\n",
        "CS 344  \n",
        "Ridge DeJong\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk4vn5FggVY9",
        "colab_type": "text"
      },
      "source": [
        "### *Vision*\n",
        "This project revolves around Deep Dream, which is in the category of image processing. It was originally created by Google, and its purpose was to help get a better understanding of how deep neural networks see images. Deep dream works by reversing the neural network so that instead of asking the model to identify an object in an image, you identify an object and tell the model to find that object in the image. If you do this to an image that doesn't contain the identified object, the model will still try to find that object because you said it is there. This will cause the model to start recreating the image in a way that it creates the illusion that the object is there. The significance of this is that when a model makes these changes, we can visually see the small pieces that the model is looking for, indicating what the model knows (and doesn't know) about that object and what direction the model needs to go to become more accurate. Therefore, the purpose of this project was open up the hood of Deep Dream to better understand how it works and what its capabilities are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHGOQ7D-ga5h",
        "colab_type": "text"
      },
      "source": [
        "### *Background*\n",
        "\n",
        "This work was based off the Deep Dream tutorial by Chollet (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb). Similar to the tutorial, this project used a pre-trained convolutional neural network called Inception_v3 as the model. A CNN was chosen above other neural networks because this type of network excels at  processing images efficiently and accurately. This CNN was trained on ImageNet, which is a database composed of more than a million hand-labeled images. Other technologies involved in this program were activation and loss of different layers (https://github.com/kvlinden-courses/cs344-code/blob/master/u08features/backpropagation.ipynb), along with a gradient ascent process (https://developers.google.com/machine-learning/glossary#gradient-descent) which have all been seen before in class (we studied gradient descent but gradient ascent is the same thing, just maximizing loss instead of minimizing). Some new technologies were preprocessing and deprocessing images, as well as detail injection into images. The point of preprocessing an image is to prepare it for the model so that it is easier to analyze. In this case, preprocessing meant resizing the image and formatting it into an appropriate tensor. Deprocessing is of course the opposite of this. After the model has finished analyzing the tensor, the tensor is converted and resized back into an image so that the results can be seen. Detail injection occurs between octaves where the image is upscaled. Since a smaller image would lose detail and become blurry when scaled bigger, details from the original image are injected to counteract this. It works by calculating the difference between the original image resized to the smaller image and the original image resized to the upscaled image. This difference therefore quantifies the details lost when going from the smaller image to the upscaled image, which can then be injected. The heart of this Deep Dream algorithm is shown below, while the full code can be found in the repo under \"Deep Dream Code\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vum6eu2GITs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill this to the path to the image you want to use\n",
        "base_image_path = '/dog.jpg'\n",
        "\n",
        "# Load the image into a Numpy array\n",
        "img = preprocess_image(base_image_path)\n",
        "\n",
        "# We prepare a list of shape tuples\n",
        "# defining the different scales at which we will run gradient ascent\n",
        "original_shape = img.shape[1:3]\n",
        "successive_shapes = [original_shape]\n",
        "for i in range(1, num_octave):\n",
        "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "    successive_shapes.append(shape)\n",
        "\n",
        "# Reverse list of shapes, so that they are in increasing order\n",
        "successive_shapes = successive_shapes[::-1]\n",
        "\n",
        "# Resize the Numpy array of the image to our smallest scale\n",
        "original_img = np.copy(img)\n",
        "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
        "\n",
        "for shape in successive_shapes:\n",
        "    print('Processing image shape', shape)\n",
        "    img = resize_img(img, shape)\n",
        "    img = gradient_ascent(img,\n",
        "                          iterations=iterations,\n",
        "                          step=step,\n",
        "                          max_loss=max_loss)\n",
        "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
        "    same_size_original = resize_img(original_img, shape)\n",
        "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
        "\n",
        "    img += lost_detail\n",
        "    shrunk_original_img = resize_img(original_img, shape)\n",
        "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
        "\n",
        "save_img(img, fname='final_dream.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOQw8Z9gqkSg",
        "colab_type": "text"
      },
      "source": [
        "### *Implementation*\n",
        "\n",
        "The project started by copying the tutorial mentioned above and getting it to run. Then, different types of images were fed to the model to see what the Deep Dream program did to them. There was a colorful image, a blank image, a static image, a high resolution image, a similar low resolution image, images with animals, a cartoon animal image, a clear image with lots of people, and a blurry image of people. With all these images, the first modification was to adjust some or all of the 11 layer activation coefficients labeled mixed0 through mixed 10. These coefficients specified how much of each layer's activation contributed to the loss being maximixed. The chunk of code where this was done is shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PRtmz1iGsMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_contributions = {\n",
        "    'mixed0': 0.001,\n",
        "    'mixed1': 0.05,\n",
        "    'mixed2': 0.5,\n",
        "    'mixed3': 1,\n",
        "    'mixed4': 1,\n",
        "    'mixed5': 0.5,\n",
        "    'mixed6': 0.1,\n",
        "    'mixed7': 0.05,\n",
        "    'mixed8': 0.005,\n",
        "    'mixed9': 0.0001,\n",
        "    'mixed10': 0.0001,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rPvu27UGpjD",
        "colab_type": "text"
      },
      "source": [
        "Second, for all these images the hyperparameters were adjusted. These included 'step' which was the gradient ascent step size, 'num_octave' which was the number of scales that the gradient ascent was run for, 'octave_scale' which was the size ratio between these different scales, 'iterations' which was the number of ascent steps for each scale, and lastly 'max_loss' which was the limit at which the gradient ascent process would terminate if reached. By using many different combinations of these hyperparameters over a diverse group of images, a good understanding of Deep Dream was gained, which is explained in the Results section. The code below shows the hyperparameters that were set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY9EviwsIlRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step = 0.05  # Gradient ascent step size\n",
        "num_octave = 5  # Number of scales at which to run gradient ascent\n",
        "octave_scale = 1.6  # Size ratio between scales\n",
        "iterations = 20  # Number of ascent steps per scale\n",
        "\n",
        "# If our loss gets larger than 10,\n",
        "# we will interrupt the gradient ascent process, to avoid ugly artifacts\n",
        "max_loss = 60."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh3QF1G4z2Yk",
        "colab_type": "text"
      },
      "source": [
        "lower level = geometric pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIItk_7-KrNK",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/moon3.jpg": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "25d3e057-b5ef-4b87-be67-741f6e054609"
      },
      "source": [
        "%%html\n",
        "<img src=\"moon3.jpg\" width=\"800\" />"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"moon3.jpg\" width=\"800\" />"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}